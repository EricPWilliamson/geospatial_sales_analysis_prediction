{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Predicting the Geography of Online Sales in Emerging Markets\nThe objective of this kernel is to create a model that is capable of predicting sales volumes with respect to geolocation. This could potentially be used by future E-Commerce ventures to predict consumer behavior in other emerging markets outside of Brazil, before the business even begins accepting orders.\n\n## Methodology\nThe Brazilian E-Commberce Public Dataset by Olist will be used to provide the total sales volume at a number of geospatial points. Additional economic and infrastructure data will be brought in from the following sources:\n- Airport geospatial data from anac.gov.br\n- Airport cargo stastistics from infraero.gov.br\n- City geospatial and population data from ???????\n- Regional economical and population data from ???????\n\nThe listed data sources will be merged to obtain several features describing each geospatial point, and those features will be used to train a model to predict sales volumes at any given geospatial location.\n\nTo begin, we look at the Olist geolocation dataset."},{"metadata":{"trusted":true,"_uuid":"d7596bc20363a9e4e1a3a81f440f6acbd65f3569"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\n###Get a geospatial description of the zip codes:\ngeo_df = pd.read_csv(\"../input/olist_geolocation_dataset.csv\", dtype={'geolocation_zip_code_prefix': str})\ngeo_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e756c179c74a43423a1df050a81558253c6bd7d2"},"cell_type":"markdown","source":"While the geolocation dataset provides us with multiple locations for each zip code, we can only use one location for each zip code when we merge with the other data. The most obvious approach is to take the mean location for each zip code and use that. This will work fine assuming that each zip code spans a small area. We can check this assumption just by using the standard deviation."},{"metadata":{"trusted":true,"_uuid":"22d9cbae70da8bce8d0ad3af591e4b00551a90d4"},"cell_type":"code","source":"# x, y = webm(geo.geolocation_lng, geo.geolocation_lat)\n# geo_df['x'] = pd.Series(x)\n# geo_df['y'] = pd.Series(y)\n#Look at each unique zip code individually:\nunique_zips = np.unique(geo_df['geolocation_zip_code_prefix'].values)\ngeo_df.loc['geolocation_zip_code_prefix'=='01001']\ngeo_grp = geo_df.groupby(['geolocation_zip_code_prefix']).agg(['mean','median','std','count'])\ngeo_grp.sort_values(by=('geolocation_lat','std'), ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b208d2abc7764705c30e28e1b3f34602852697ab"},"cell_type":"markdown","source":"...and clearly something is wrong here. Some of these zip codes have astronomically high standard deviations, meaning they contain points sprawled over a huge area. We can look at one example to see why."},{"metadata":{"trusted":true,"_uuid":"3fe324eeac0d1f80caa4eab8f16c3fc48781663b"},"cell_type":"code","source":"###Look at the points within one of the bad zip codes:\nBAD_ZIP = ['28595']\nplot_df = geo_df.loc[geo_df['geolocation_zip_code_prefix'].isin(BAD_ZIP)].copy()\nplot_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2595fdd44dd5962390b04c4b672da73065c3e5e"},"cell_type":"markdown","source":"We can see that we have one major outlier here. And after manually inspecting a few others of these zip codes, I can see that outliers are not so uncommon. \n\nSince many of these zip codes contain so few points, and we can't guarentee that we only have a single outlier, outlier detection is not always possible. The safest approach is to discard any zip codes that only contain one point, and also discard zip codes with small numbers of points and high standard deviations. Once those zip codes have been filtered out, any outliers in other sets can be neutralized by using the median location, rather than the mean."},{"metadata":{"trusted":true,"_uuid":"ceeee62e559504f9ef16b8d0bc12845d605965d5"},"cell_type":"code","source":"print('Total number of zip codes:                                               {}'.format(geo_grp.shape[0]))\nfilt_1 = geo_grp['geolocation_lat','count']==1\ngeo_grp = geo_grp.loc[~filt_1]\nprint('Number of zip codes with only one point:                                 {}'.format(filt_1.sum()))\nfilt_2 = (geo_grp['geolocation_lat','count']<5) & ((geo_grp['geolocation_lat','std']>0.1) | (geo_grp['geolocation_lng','std']>0.1))\ngeo_grp = geo_grp.loc[~filt_2]\nprint('Number of zip codes with less than 5 points and high standard deviation: {}'.format(filt_2.sum()))\nprint('Remaining number of zip codes:                                           {}'.format(geo_grp.shape[0]))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ef50f84ad5fecc198892205951cd564fe57507d"},"cell_type":"markdown","source":"We see that even after applying a rather conservative filter to eliminate questionable data, we are still left with 17857 usable zip codes."},{"metadata":{"_uuid":"ebd928a58f81d6a62fdf65f91672ea253e1adac7"},"cell_type":"markdown","source":"## Brazil's airports\n\nThe size of and distance to the nearest airport is indicitive of existing economic conditions in an area, and also the cost of shipping to that area. All of these implications will influence local customers' potential spending habits.\n\nTo utilize these data, we first need to download and clean the data. Then merge the data. Later, we can compare customers' locations to the airport locations to develop features for our training model.\n\nWe begin with the airport cargo statistics data."},{"metadata":{"trusted":true,"_uuid":"e92f64b6590a46015e94d3de81bbe882e8e6119f"},"cell_type":"code","source":"#Download cargo statitics for Brazil's airports\nimport requests\nr = requests.get('http://www4.infraero.gov.br/media/673983/jun.xlsx')\nif r.status_code == 200:\n    with open('jun.xlsx','wb') as f:\n            for chunk in r.iter_content(1024):\n                f.write(chunk)\n# os.stat('./jun.xlsx')\ncargo_df = pd.read_excel('./jun.xlsx', sheet_name=1, header=4, usecols='B,H').dropna()\ncargo_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4db3716e7d75f493c11297767e0bd59f54312c68"},"cell_type":"code","source":"###Clean up the cargo data\n#Replace the column names with english equivalents:\ncargo_df.columns = ['Description', 'Cargo loaded and unloaded per year']\n#Keep only the main row for each airport:\nmatches = cargo_df['Description'].str.contains('[A-Z]{4} - ')\ncargo_df = cargo_df[matches]\n#Separate the airport codes from the common names\ncargo_df['ICAO code'], cargo_df['Airport name'] = cargo_df['Description'].str.split(' - ').str\ncargo_df = cargo_df.drop(columns=['Description'])\n#Reorder the columns:\ncargo_df = cargo_df.reindex(columns=['ICAO code','Airport name', 'Cargo loaded and unloaded per year'])\ncargo_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53c4e3130e9bb8ef1c2d751a4b736ee8e42d16b8"},"cell_type":"markdown","source":"Next, we move on to the airport geolocation data."},{"metadata":{"trusted":true,"_uuid":"8f6e5dd16e6b186155503a365cbd1c1fa73d9e04"},"cell_type":"code","source":"!wget http://www2.anac.gov.br/arquivos/pdf/aerodromos/AerodromosPublicos.xls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a79de7265de8997b8acfa44e7430d3331f524038"},"cell_type":"code","source":"import re\n###Clean up the airport location data\nairport_df = pd.read_excel('./AerodromosPublicos.xls', header=1, usecols='A,F,G')\n#Replace the column names with english equivalents:\nairport_df.columns = ['ICAO code', 'Latitude_str','Longitude_str']\n#Convert lat/lon strings to decimal values:\ndef latlon_get_decimal(input_str):\n    X = re.search(\"(\\d+)(?:°\\s*)(\\d+)(?:'\\s*)(\\d+)(?:''\\s*)([A-Z])\", input_str)\n    dval = int(X.group(1)) + int(X.group(2))/60 + int(X.group(3))/3600\n    if X.group(4) in ['S','W']:\n        dval = dval * -1\n    return dval\nairport_df['Lat'] = airport_df['Latitude_str'].apply(latlon_get_decimal)\nairport_df['Lon'] = airport_df['Longitude_str'].apply(latlon_get_decimal)\nairport_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed8816a10ae538bd471e10ae9cfd465f6de84d52","scrolled":true},"cell_type":"code","source":"###Merge cargo data with airport location data:\nairportcargo = cargo_df.merge(airport_df, on='ICAO code')\nairportcargo.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d701b68aab2f1f25e7aa7bc6a80d3e08e43aacef"},"cell_type":"markdown","source":"## Brazil's States\nBrazil's 27 states (officially called federative units) each cover a vast area. With the largest, Amazonas, being nearly half the size of India. While the large areas are not ideal for our purposes, the detailed economic data at this level makes them useful nonetheless.\n\nI found a number of good resources showing various data at the state level, but to try something a little different, we will scrape a table of data from __[this Wikipedia page.](https://en.wikipedia.org/wiki/States_of_Brazil)__\n\nPandas' `read_html()` method makes this very convenient."},{"metadata":{"trusted":true,"_uuid":"4b3415341106043865fd4c2e396962d7760d680c"},"cell_type":"code","source":"#Scrape the raw html source from the webpage:\nhtml = requests.get(\"https://en.wikipedia.org/wiki/States_of_Brazil\").text\n#Use pandas to parse the table from the html source\nstate_df = pd.read_html(html,attrs = {'class':\"wikitable sortable\"}, header=0, flavor='bs4')[0]\n#Remove some unwanted columns:\nstate_df = state_df.drop(['Flag','Capital','Area (sq mi)','Density (per sq mi, 2017)'], axis=1)\n#This column contains two values per cell, but we only want the first one:\nstate_df['GDP (billion R$, 2012)'] = state_df['GDP (billion R$ and\\xa0% total, 2012)'].apply(\n    lambda x: float(x.split('(')[0].replace(',','')))\nstate_df = state_df.drop('GDP (billion R$ and\\xa0% total, 2012)', axis=1)\n#These columns contain percentages. Convert them to rate values:\nstate_df['Literacy (2014)'] = state_df['Literacy (2014)'].apply(\n    lambda x: float(x.split('%')[0])/100)\nstate_df['Infant mortality (2014)'] = state_df['Infant mortality (2014)'].apply(\n    lambda x: float(x.split('%')[0])/100)\nstate_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74b9a4e8ffb58fd289b9eee068dfd0b844d3aa27"},"cell_type":"markdown","source":"Later on, we will also need a way to correlate state abbreviations with their full names. This dataframe gives us a convenient opportunity to create a dictionary object with this functionality."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b8c0ee4b3638f58bc82790d3e22424b00bd8071a"},"cell_type":"code","source":"state_abbr = dict(np.fliplr(state_df[['Federative unit', 'Abbreviation']].values))\nstate_abbr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d90eed9df70037a9188cc393401800071455104"},"cell_type":"markdown","source":"## Brazil's Cities\nAn area's economy will be dependent on its distance to the nearest major city center, and also the level of economic development within that city. Unfortunately, I was unable to locate publicly available data describing economic develpment across Brazil's cities, so the population of each city will be used to infer economic development. This is obviously non-ideal, but we will try to make do.\n\n"},{"metadata":{"trusted":true,"_uuid":"4628457666d433762eeb916da7699f9d543182c4"},"cell_type":"code","source":"!wget ftp://ftp.ibge.gov.br/Estimativas_de_Populacao/Estimativas_2017/estimativa_TCU_2017_20181108.xls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c814736edd516bc77a7c395f8bb6c2f8a3f28dc"},"cell_type":"code","source":"city_df = pd.read_excel('./estimativa_TCU_2017_20181108.xls', sheet_name=1, header=1,\n                       dtype=str)\ncity_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97115228ed56562d63201c487452cc5dfa2bc795"},"cell_type":"markdown","source":"This table needs some clean-up as well. In particular, we see some irregularities in numbers of the final column."},{"metadata":{"trusted":true,"_uuid":"6fb216c26d07172a5aacba293f1769696262db2e"},"cell_type":"code","source":"#Replace the column names with english equivalents:\ncity_df.columns = ['State', 'State code','City code','Municipality','Estimated population']\n#Remove irregularities in the population numbers:\ndef get_population_number(input_str):\n    input_str = input_str.split('(')[0]\n    input_str = input_str.replace('.','')\n    try:\n        return int(input_str)\n    except ValueError:\n        return np.nan\ncity_df['Estimated population'] = city_df['Estimated population'].apply(get_population_number)\ncity_df = city_df.dropna()\n#We dont really have any use for the state and city codes, so drop them:\ncity_df = city_df.drop(['State code','City code'], axis=1)\ncity_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38af974e39208543bf40fdbbac2398891ca5d605"},"cell_type":"markdown","source":"I was unable to find tabulated data showing the geoposition of each city. So instead we will grab that data from Wikipedia using the very convenient wikipedia package.\n\nAt first I attempted to search for each city name in sucession, but this method is slow and produces many irregular results (such as in cases where there are several cities sharing the same name). A far better method is to use [this wikipedia page](https://en.wikipedia.org/wiki/List_of_municipalities_of_Brazil), which tabulates the links to the articles on every Brazilian municipality."},{"metadata":{"trusted":true,"_uuid":"f98259d70a3863de5837523b574ed218fc63ab77"},"cell_type":"code","source":"from bs4 import BeautifulSoup\n#Scrape the raw html source from the webpage:\nhtml = requests.get(\"https://en.wikipedia.org/wiki/List_of_municipalities_of_Brazil\").text\n#read_html() can only get the text from the table:\nlinks_df = pd.read_html(html,attrs = {'class':'wikitable sortable'}, header=0, flavor='bs4')[0]\n#Rather than keeping the entire state name, it will be more convenient to only keep the abbreviation:\nlinks_df['State'] = links_df['State'].apply(\n    lambda x: x.split('(')[1].split(')')[0])\n#Remove the columns that aren't useful to us:\nlinks_df = links_df.drop(['Mesoregion','Microregion'], axis=1)\n\n###Use beautifulsoup to parse hrefs from the table:\nb = BeautifulSoup(html,'lxml')\ntable = b.find(name='table')\ntable_cells = np.array([td for td in table.find_all(name='td')])\n#Take the links from every 4th cell in the table:\nmuni_links = np.array([td.a['href'] for td in table_cells[3::4]])\n#Add the links to our dataframe:\nlinks_df['Link'] = muni_links\nlinks_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a37777fa24c1ae7035f00772d63247d4a2f9ce30"},"cell_type":"code","source":"###Merge the city population dataframe with the links:\n#Change the capitalization of names in city_df so that they match the wikipedia names:\ncity_df['Municipality'] = city_df['Municipality'].apply(\n    lambda x: x.replace(\"D'\", \"d'\"))\n#Try the merge, but check which cities didn't match up with a wikipedia link:\ncity_df = city_df.merge(links_df, how='left', on=['State','Municipality'])\nunmatched = city_df.loc[pd.isnull(city_df['Link'])]\nprint('{} cities did not match a wikipedia entry'.format(unmatched.shape[0]))\nunmatched.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"a43625c37ae16a6b72816b3f03ca28e02c682535"},"cell_type":"code","source":"###Use difflib for fuzzy string matching:\nimport difflib\n\ndef remove_accents(input_str):\n    input_str = input_str.replace('á', 'a')\n    input_str = input_str.replace('é', 'e')\n    input_str = input_str.replace('ê', 'e')\n    input_str = input_str.replace('í', 'i')\n    input_str = input_str.replace('ó', 'o')\n    input_str = input_str.replace('ú', 'u')\n    return input_str\n\nfor idx,row in city_df.loc[pd.isnull(city_df['Link'])].iterrows():\n    #Take the portion of the links_df that matches the state\n    choices = links_df.loc[links_df['State']==row['State']]\n    #difflib seems to struggle to match strings with different accent marks, so remove them:\n    city_name = remove_accents(row['Municipality'])\n    choices['Municipality'] = choices['Municipality'].apply(remove_accents)\n    #Find which choice best matches the unfinished row:\n    match = difflib.get_close_matches(city_name, \n                                      choices['Municipality'].values, \n                                      n=1, \n                                      cutoff=0.8)\n    if match:\n        city_df.loc[idx,'Link'] = choices.loc[choices['Municipality']==match[0],'Link'].values[0]\n#Check again and see how many cities are still yet to find a match:\nunmatched = city_df.loc[pd.isnull(city_df['Link'])]\nprint('{} cities did not match a wikipedia entry'.format(unmatched.shape[0]))\nunmatched.head(50)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14be0aca9a4b0f95fa13f855b530f45cbcdd7bb5"},"cell_type":"markdown","source":"Now we're down to just 18 cities without a match. Not bad considering that we started with over 5000 cities.\n\nThe remaining cities can be matched by using the search function in the wikipedia API."},{"metadata":{"trusted":true,"_uuid":"3d5576ed0ad28c2e207281216e2d5d63005afd2b","scrolled":false},"cell_type":"code","source":"###Scrape city data from Wikipedia.org\nimport wikipedia\nwikipedia.set_lang(\"en\")\n\n#Search for each city that still hasn't found a match:\nfor idx,row in city_df.loc[pd.isnull(city_df['Link'])].iterrows():\n    search_str = row['Municipality'] + ', ' + state_abbr[row['State']]\n    page = wikipedia.page(search_str)\n    print(search_str)\n    print(page.title)\n    print(page.url)\n\n# \"\"\"\n# We need to look at the individual list for each state (eg. /wiki/List_of_municipalities_in_Amap%C3%A1)\n#  and then find the corresponding municipality within that webpage. That link is the most reliable way \n#  to arrive at the correct wikipedia article.\n# \"\"\"\n# muni = wikipedia.page('Municipalities of Brazil')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"726adb0e359e679da8bafe1e80bf86cf499089de"},"cell_type":"code","source":"# def get_city_coords(city_name):\n#     #Some of the names in city_df appear like \"Machadinho D'Oeste\", \n#     #  while wikipedia changes the capitialization to \"Machadinho d'Oeste\"\n#     city_name = city_name.replace(\"D'\",\"d'\")\n#     try:\n#         muni = wikipedia.page(city_name, auto_suggest=False)\n#         return float(muni.coordinates[0]), float(muni.coordinates[1])\n#     except wikipedia.DisambiguationError:\n#         print('---DisambiguationError---')\n#         print(city_name)\n#     except KeyError:\n#         print('---KeyError---')\n#         print(city_name)\n#     except wikipedia.PageError:\n#         print('---PageError---')\n#         print(city_name)\n#     return np.nan, np.nan\n# city_df['Lat','Lon'] = city_df['City name'].apply(get_city_coords)\n# city_df","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}